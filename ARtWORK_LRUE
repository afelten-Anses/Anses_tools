#!/global/conda/envs/artwork/bin/python
# -*- coding: iso-8859-1 -*-
import os, sys
import subprocess 
import datetime
import time
import argparse
import glob
from pyunpack import Archive 	
import shutil
from shutil import copyfile
import smtplib
from Bio import SeqIO
import patoolib
import re


__doc__="""
@requires: BWA
@requires: samtools
@requires: picard-tools
@requires: Trimmomatic
@requires: GATK
@requires: SnpSift
@requires: iVARCall2
@requires: pigz
@requires: tabix
@requires: vcf-merge
@requires: Vcf.pm (vcftools)
@requires: bbnorm
@requires: bbmap
@requires: fastqc
@requires: mlst
@requires: spades
@requires: quast
@requires: medusa
@requires: mash
@requires: gmcloser
@requires: prokka
@requires: seqsero
"""

def get_parser():
	"""
	Parse arguments
	@return: arguments list
	@rtype: parser object
	"""

	parser = argparse.ArgumentParser(description='run reads analysis, variant analysis, assembly and annotation \
		from raw reads and add outputs in the GAMeR database')

	parser.add_argument('-1', action="store", dest='reads1',
						type=str, required=True, help='Fastq file pair 1 with \'_R1\' (REQUIRED)')

	parser.add_argument('-2', action="store", dest='reads2',
						type=str, required=True, help='Fastq file pair 2 with \'_R2\' (REQUIRED)')

	parser.add_argument('-r1', action="store", dest='RefVariant',
						type=str, required=True, help='reference used for variant calling (REQUIRED)')

	parser.add_argument('-r2', action="store", dest='RefScaffold',
						type=str, required=True, help='reference used for scaffolding (REQUIRED)')

	parser.add_argument('-ad', action="store", dest='ADAPTATERS',
						type=str, required=True, help='Sequencing adaptaters file (REQUIRED)')

	parser.add_argument('-minCov', action="store", dest='minCov',
						type=int, default=30, help='minimum coverage required for workflow (default:30)')

	parser.add_argument('-targetCov', action="store", dest='targetCov',
						type=int, default=100, help='minimum coverage required for normalization, 0 for no normalization (default:100)')

	parser.add_argument('-l', action="store", dest='minContigLen',
						type=int, default=200, help='minimum contig length (default:200)')

	parser.add_argument('-p', action="store", dest='minPhredScore',
						type=int, default=30, help='minimum phred score (default:30)')

	parser.add_argument('-T', action="store", dest='nbThreads', 
						type=int, default=1, help='maximum number of threads to use (default:1)')

	parser.add_argument('-m', action="store", dest='maxMemory', 
						type=int, default=4000, help='max memory to use in Mb (default:4000)')

	parser.add_argument('-w', action="store", dest='workdir', 
						type=str, default='.', help='working directory (default:current directory)')

	parser.add_argument('-a', action="store", dest='min_mapped_scaffold_percent', 
						type=int, default=80, help='minimum percent of total scaffold length \
						mapped on reference, used to detect contaminants (default:80)')

	parser.add_argument('-Ksize', action="store", dest='Ksize', 
						type=str, default='15', help='Kmer size for mash (default:15)')	

	parser.add_argument('-Ssize', action="store", dest='Ssize', 
						type=str, default='1000', help='Sketch size for mash (default:15)')										

	parser.add_argument('-d', action="store", dest='max_len_diff', 
						type=int, default=30, help='maximum percent of length difference \
						between assembly and reference (default:20)')

	parser.add_argument('-medusa', action="store", dest='medusa_path', type=str, help='medusa jar path ')
        
	parser.add_argument('-medusa_scripts', action="store", dest='medusa_scripts_path', type=str, help='medusa scripts path')

	parser.add_argument('-GATK', action="store", dest='GATK_path',type=str, help='GATK jar path')


	return parser



class sample(object) :


	def __init__(self, SampleID, report, \
		reads1Path, reads2Path, readsLen, reads1QC, reads2QC, \
		assembly, contigs, QUAST, GBK, GFF, \
		VCF, sketch, nbContigs, totalLength, \
		largestContig, n50, genomeFraction, refScafold, refSNP, breadthCoverage):
		"""
		Initialize the sample class
		"""

		self.id = SampleID	
		self.report = report

		self.nbReads = number_of_reads(reads1Path)
		self.reads1 = self.compressReads(reads1Path)
		self.reads2 = self.compressReads(reads2Path)
		self.readsLen = readsLen
		self.reads1QC = reads1QC
		self.reads2QC = reads2QC
		self.VCF = VCF
		self.RefSNP = refSNP
		self.BreadthCoverage = breadthCoverage

		self.assembly = assembly
		self.contigs = contigs
		self.QUAST = QUAST
		self.GBK = GBK
		self.GFF = GFF
		self.sketch = sketch
		self.NbContigs = nbContigs
		self.TotalLength = totalLength
		self.LargestContig = largestContig
		self.N50 = n50
		self.GenomeFraction = genomeFraction
		self.RefScafold = refScafold



	def moveFiles(self, FINAL_OUTPUT_DIRECTORY):

		mongo_output = '/'.join(FINAL_OUTPUT_DIRECTORY.split('/')[4:]) + '/'

		os.system("mv " + self.report + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")

		self.report = mongo_output + self.report.split("/")[-1]
		
		os.system("mv " + self.reads1 + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads1 = mongo_output + self.reads1.split("/")[-1]

		os.system("mv " + self.reads2 + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads2 = mongo_output + self.reads2.split("/")[-1]

		os.system("mv " + self.reads1QC + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads1QC = mongo_output + self.reads1QC.split("/")[-1]

		os.system("mv " + self.reads2QC + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.reads2QC = mongo_output + self.reads2QC.split("/")[-1]

		os.system("mv " + self.assembly + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.assembly = mongo_output + self.assembly.split("/")[-1]

		os.system("mv " + self.contigs + ' ' + FINAL_OUTPUT_DIRECTORY + "/" + self.id + "_contigs.fasta")
		self.contigs = mongo_output + self.id + "_contigs.fasta"

		os.system("mv " + self.QUAST + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.QUAST = mongo_output + self.QUAST.split("/")[-1]

		os.system("mv " + self.GBK + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.GBK = mongo_output + self.GBK.split("/")[-1]

		os.system("mv " + self.GFF + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.GFF = mongo_output + self.GFF.split("/")[-1]

		os.system("mv " + self.VCF + '* ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.VCF = mongo_output + self.VCF.split("/")[-1]

		os.system("mv " + self.sketch + ' ' + FINAL_OUTPUT_DIRECTORY + "/.")
		self.sketch = mongo_output + self.sketch.split("/")[-1]


	def compressReads(self, reads):	
		if os.path.basename(reads).split('.')[-1] != "gz" :
			os.system("pigz " + reads)
			reads = reads + ".gz"

		return reads	


def Get_maxMemoryg(maxMemory):

    #1go to 10go RAM conversion
    if(int(maxMemory) >= 1000 and int(maxMemory) < 10000):
        maxMemoryg = "-Xmx" + list(str(maxMemory))[0] + "g"

    #10go to 100go RAM conversion
    elif(int(maxMemory) >= 10000 and int(maxMemory) < 100000):
        maxMemoryg = "-Xmx" + str(list(str(maxMemory))[0] + list(str(maxMemory))[1]) + "g"

    #100go to 1000go RAM conversion
    elif(int(maxMemory) >= 100000 and int(maxMemory) < 1000000):
        maxMemoryg = "-Xmx" + str(list(str(maxMemory))[0] + list(str(maxMemory))[1] + list(str(maxMemory))[2]) + "g"

    #1To to 10To RAM conversion
    elif(int(maxMemory) >= 1000000 and int(maxMemory) < 10000000):
        maxMemoryg ="-Xmx" + str(list(str(maxMemory))[0] + list(str(maxMemory))[1] + list(str(maxMemory))[2] + list(str(maxMemory))[3]) + "g"

    #1go is the minimum requierement to be able to some tools, so if arguments.nbThreads<1go, enter Arguments.nbThreads converted defaut value
    else:
        maxMemoryg="-Xmx4g"
        
    return maxMemoryg
	

def check_finalDirectory_exist(FINAL_OUTPUT_DIRECTORY, logFilepath):
	
	logFile = open(logFilepath, "a")
	logFile.write("Check if final directory exists...")

	if(os.path.isdir(FINAL_OUTPUT_DIRECTORY) ==True):

		Gid = FINAL_OUTPUT_DIRECTORY.split('/')[-1]
		message = "ERROR:" + Gid + " already exist in " + FINAL_OUTPUT_DIRECTORY 

		#add msgerr in logFile
		logFile = open(logFilepath, 'a')
		logFile.write(message + '\n') 
		logFile.close()
		sys.exit(1)

	logFile.write("...ok\n")
	logFile.close()


def uncompress_in_workdir(CompressedRead,logFilepath,workdir):


	################################################################
	## Extract reads_filename and file path from compressed file  ##
	################################################################

	os.system('echo "\n************************** READS UNPACKING : START **************************\n" >> '+ logFilepath)

	#File found
	if(os.path.exists(CompressedRead)):

		reads_filename = '.'.join(CompressedRead.split('.')[0:2])
		reads_filename = reads_filename.split('/')[-1]
		reads_filepath = workdir + '/' + reads_filename

	#File not found
	else:
		logFile = open(logFilepath, 'a')
		logFile.write("File not found : " + CompressedRead)
		logFile.close()
		sys.exit(1)
	

	##################################################
	##    Test and unpack archive if test is ok     ##
	##################################################

	### TEST ###

	os.system('echo "unpacking started at :" >> ' + logFilepath + ' 2>&1 | date >> ' + logFilepath + ' 2>&1')

	try:
		patoolib.test_archive(CompressedRead,verbosity=0)

	except:
		logFile = open(logFilepath, 'a')
		logFile.write("Problem with file : " + CompressedRead)
		os.system('echo "unpacking stopped with status error 1 at" : >> ' + logFilepath + ' 2>&1 | date >> ' + logFilepath + ' 2>&1')
		is_corruputed=True
		sys.exit(1)

	
	### IF TEST=SUCESS --> UNPACK IN CURRENT WORKING DIRECTORY ###

	else:
		Archive(CompressedRead).extractall(workdir)
		is_corruputed=False

	os.system('echo "unpacking finished sucessfully at" : >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

	os.system('echo "\n************************** READS UNPACKING : END **************************\n" >> '+ logFilepath)

	return reads_filename,is_corruputed


def normalization(reads1, reads2, targetCov, nbThreads, Gid, logFilepath, reportFilepath):

	reads1_normalized = Gid + "_norm_R1.fastq"
	reads2_normalized = Gid + "_norm_R2.fastq"

	cmd = "bbnorm.sh -Xmx20g " + \
	"in=" + reads1 + " " + \
	"in2=" + reads2 + " " + \
	"target=" + str(targetCov) + " " + \
	"mindepth=-1 " + \
	"maxdepth=1 " + \
	"out=" + reads1_normalized + " " + \
	"out2=" + reads2_normalized + " " + \
	"outt=trashReads.fastq " + \
	"threads=" + str(nbThreads) + " " + \
	"prefilter=t " + \
	"tossbadreads=t" 

	logFile = open(logFilepath, "a")
	logFile.write("\nReads Normalization to " + str(targetCov) + "X\n")
	logFile.write(cmd + "\n")
	logFile.close()

	os.system('echo "bbnorm started at : " >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)
	os.system(cmd)
	os.system('echo "bbnorm ended at" : >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

	report_file = open(reportFilepath, "a")
	report_file.write("TARGET COVERAGE FOR NORMALIZATION : " + str(targetCov) + 'X\n\n')
	report_file.close()
	
	return reads1_normalized, reads2_normalized
    

def Check_coverage(reads1, reads2, minCov, nbThreads, maxMemory, workdir, logFilepath, reportFilepath, reference):
    
    os.system('echo "\n************************** QUALITY CONTROL (Check coverage) : START **************************\n" >> '+ logFilepath)

    #Convert maxMemory argument : Aaaa int to to -XmxAg form
    Maxmemoryg=Get_maxMemoryg(maxMemory)

    logFile = open(logFilepath, "a")
    logFile.write("Retrieving default reference...")

    logFile.write("...ok\n")

    os.system('echo "bbmap started at : " >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

    ##################################################
    ## Map the reads and generate its quality stats ##
    ##################################################
    cmd = "bbmap.sh " +\
    "in="+reads1+" "+\
    "in2="+reads2+" "+\
    "ref="+reference+" "+\
    "covstats=" + workdir +"/stats/covstats.txt "+\
    "covhist=" + workdir + "/stats/covhist.txt "+\
    "bincov=" + workdir + "/stats/bincov.txt "+\
    "rpkm=" + workdir + "/stats/rpkm_fpkm.txt "+\
    "ehist=" + workdir + "/stats/error_hist "+\
    "statsfile=" + workdir + "/stats/reads_summary.txt "+\
    "covbinsize=1000 " + \
    "k=13 "+\
    "threads="+str(nbThreads)+" "+\
    "nodisk "+\
    str(Maxmemoryg)+" "+\
    ">> " + str(logFilepath) + " 2>&1"

    logFile.write("\n" + cmd + "\n")
    os.system(cmd)

    os.system('echo "bbmap ended at" : >> ' + logFilepath + " 2>&1 | date >> " + logFilepath)

    #####################
    ## Quality control ##
    #####################
    logFile.write("quality control : check medium coverage")
    #si coverage < minCov ---> écrire message d'erreur dans stdin + report + os.exit(1)
    #ajouter coverage trouvé + std variantion dans report
    coverage=subprocess.check_output("cat " + workdir + "/stats/covstats.txt | cut -f2 | sed -n \"2 p\"", shell=True).rstrip().replace(',','.')
    coverage_std_var=subprocess.check_output("cat " + workdir + "/stats/covstats.txt | cut -f11 | sed -n \"2 p\"", shell=True)
    
    report_file = open(reportFilepath, "a")
    report_file.write("READS DEEP COVERAGE : " + coverage + '\n')
    report_file.write("COVERAGE STANDARD DEVIATION : " + coverage_std_var + "\n")
    report_file.close()

    if (float(coverage) < float(minCov)):
        logFile.write("\n Warning, too low coverage : " + coverage + "x")
        logFile.close()
        sys.exit(1)

    logFile.write("...ok\n")
    logFile.write("RPKM / FPKM : \n")
    os.system("cat stats/rpkm_fpkm.txt >> " + logFilepath)
    logFile.write("\nCoverage histogram : \n")
    os.system("cat stats/covhist.txt >> " + logFilepath)
    logFile.close()

    os.system('echo "\n************************** QUALITY CONTROL (Check coverage) : END **************************\n" >> '+ logFilepath)

    return int(float(coverage))


def soft_trimming(reads1, reads2, adaptaters, nbThreads, maxMemory, phredscore, Gid, workdir, report, logFilepath):
	
	os.system('echo "************************** TRIMMING WORKFLOW : START ************************** " >> '+ logFilepath)

	Maxmemoryg=Get_maxMemoryg(maxMemory)
	logFile = open(logFilepath, "a")

	##################################################
	## Map the reads and generate its quality stats ##
	##################################################

	logFile.write("Calculating number of reads per file...")
	nbReads_before_trimming = number_of_reads(reads1)
	logFile.write("...ok\n")

	forward_paired = workdir + "/" + Gid + "_paired_R1.fq"
	forward_unpaired = workdir + "/" + Gid + "_unpaired_R1.fq.gz"
	reverse_paired = workdir + "/" + Gid + "_paired_R2.fq"
	reverse_unpaired = workdir + "/" + Gid + "_unpaired_R2.fq.gz"

	os.system('echo "Trimmomatic started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	command = "trimmomatic " + str(Maxmemoryg) + " PE -threads " + \
	str(nbThreads) + " -phred33 " + reads1 + " " + reads2 + " " + forward_paired + " " + \
	forward_unpaired + " " + reverse_paired + " " + reverse_unpaired + \
	" ILLUMINACLIP:" + adaptaters + ":2:" + str(phredscore) + ":15 " + \
	"TRAILING:20 MINLEN:50 >> " + logFilepath + " 2>&1"

	logFile.write("\n" + command + "\n")
	os.system(command)
	os.system('echo "Trimmomatic ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	#os.system("pigz " + forward_paired + ".gz")
	#os.system("pigz " + reverse_paired + ".gz")

	os.remove(forward_unpaired)
	os.remove(reverse_unpaired)

	logFile.write("Calculating number of reads per file after trimming...")
	nbReads_after_trimming = number_of_reads(forward_paired)
	logFile.write("...ok\n")
	logFile.close()

	report_file = open(report, "a")
	report_file.write("" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+            TRIMMING RESULTS           +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n")
	report_file.write("READS TRIMMING PARAMETERS : minQual = 20 -- minLen = 50 -- minPhred = " + str(phredscore) + "\n")
	report_file.write("READS COUNT BY FILE BEFORE TRIMMING : " + str(nbReads_before_trimming) + " \n")
	report_file.write("READS COUNT BY FILE AFTER TRIMMING : " + str(nbReads_after_trimming) + " \n")
	report_file.write("TRIMMED READS COUNT : " + str(int(nbReads_before_trimming-nbReads_after_trimming)) + "\n\n")
	report_file.close()

	list_reads_trim = [forward_paired, reverse_paired]
	
	os.system('echo "\n************************** TRIMMING WORKFLOW : END **************************\n" >> '+ logFilepath)
	return list_reads_trim


def Fastqc(reads1, reads2, adaptators, outputName, nbThreads, workdir, logFilepath, reportFilepath):

	#A mettre dans fichier REPORT : lien vers le report html

	os.system('echo "\n************************** FASTQC REPORT : START **************************\n" >> '+ logFilepath)

	os.mkdir(workdir + "/" + outputName)
	os.system('echo "FastQC started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	cmd = "fastqc -q -t " + str(nbThreads) + " --contaminants " + adaptators + \
		" -o " + workdir + "/" + outputName + " " + reads1 + " " + reads2 + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	os.system(cmd)

	os.system('echo "FastQC ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	#Full path to zip files
	TXTfastqcResult_R1 = workdir + "/fastqc/" + reads1.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc/fastqc_data.txt"
	TXTfastqcResult_R2 = workdir + "/fastqc/" + reads2.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc/fastqc_data.txt"

	#Full path to txt files
	ZIPfastqcResult_R1 = workdir + "/fastqc/" + reads1.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc.zip"
	ZIPfastqcResult_R2 = workdir + "/fastqc/" + reads2.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc.zip"

	#Full path to html files
	HTMLfastqcResult_R1 = workdir + "/fastqc/" + reads1.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc"
	HTMLfastqcResult_R2 = workdir + "/fastqc/" + reads2.rsplit('/',-1)[-1].rsplit('.')[0] + "_fastqc"

	#inflate zip files
	os.system("unzip " + ZIPfastqcResult_R1 + " -d fastqc ")
	os.system("unzip " + ZIPfastqcResult_R2 + " -d fastqc ")

	listResult = [HTMLfastqcResult_R1, HTMLfastqcResult_R2, TXTfastqcResult_R1, TXTfastqcResult_R2]

	os.system('echo "\n************************** FASTQC REPORT : END **************************\n" >> '+ logFilepath)

	return listResult


def commande_iVARCAll(reads1, reads2, adaptaters, nbThreads, maxMemory, Gid, workdir, logFilepath, report, reference, GATK_path):

    os.system('echo "\n************************** iVARCall2 WORKFLOW : START **************************\n" >> '+ logFilepath)

    logFile = open(logFilepath, "a")    

    os.system('echo "iVARCall2 started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

    cmd="iVARCall2 -ref " + reference + " -reads " + reads1 + " " + reads2 + \
        " -a " + adaptaters + " -o " + Gid + "_iVARCAll -T " + str(nbThreads) + " -m " + str(maxMemory) + \
	" -GATKJAR " + str(GATK_path) + \
        " --removeDuplicates --indelRealigner --removeTmpFiles --onlyVCF" + " >> " + logFilepath + " 2>&1"
    #cmd="iVARCall2 -ref " + reference + " -reads " + reads1 + " " + reads2 + \
    #    " -a " + adaptaters + " -o " + Gid + "_iVARCAll -T " + str(nbThreads) + " -m " + str(maxMemory) + \
    #            " -TRIMJAR " + trimmomatic_path + " -GATKJAR " + GATK_path + " -PICARDJAR " + picard_path + \
    #    " --removeDuplicates --indelRealigner --removeTmpFiles --onlyVCF" + " >> " + logFilepath + " 2>&1"

    logFile.write("\n" + cmd + "\n")
    logFile.close()
    
    os.system(cmd)

    os.system('echo "iVARCall2 ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

    iVARCAll_log = Gid + "_iVARCAll/" + Gid + "_iVARCAll.log" # récupérer le log créer par iVARCall
    VCF_file = Gid + "_iVARCAll/VCF/" + Gid + ".g.vcf" # récupérer path VCF

    iVARCall_log_parser(iVARCAll_log, VCF_file, reference, report)

    #shutil.rmtree(Gid + "_iVARCAll/BAM") # remove BAM folder
    
    os.system('echo "\n************************** iVARCall2 WORKFLOW : END **************************\n" >> '+ logFilepath)

    return VCF_file
    

def iVARCall_log_parser(iVARCAll_log, VCF_file, reference, report):

	logFile = open(iVARCAll_log, "r")
	logLines = logFile.readlines()
	logFile.close()

	dicoStat = extractReadStatistics(logLines)

	report_file = open(report, "a")
	report_file.write(""+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+         iVARCALL 2 METRICS           +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n\n")

	reference = reference.split('/')[-1]
	report_file.write("REFERENCE GENOME : " + reference.split('.')[0] + '\n')
	report_file.write("BREADTH COVERAGE : " + dicoStat["breadth cov."] + '\n')
	report_file.write("TOTAL READS COUNT : " + dicoStat["reads"] + '\n')
	report_file.write("TOTAL READS COUNT AFTER TRIMMING : " + dicoStat["trimming"] + '\n')
	report_file.write("MAPPED READS : " + dicoStat["mapped"] + '\n')
	report_file.write("PAIRED MAPPED READS : " + dicoStat["paired"] + '\n')

	SNPnb = numberOfVariant_from_VCF(VCF_file)
	report_file.write("SNPs COUNT (UNFILTERED) : " + str(SNPnb) + '\n\n\n')

	report_file.close()


def extractReadStatistics(lines):
	"""
	Extract reads statistics from the log file lines.
	@param lines : lines of the log file 
	@type lines : list
	@return: dictionnary with sample name in key and for value an other \
	dictionnary with the number of reads before and after trimming, \
	the number of reads mapped and properly paired, \
	and the deep and breadth coverage.
	@rtype: dictionnary 
	"""

	dicoResult = {}

	for line in lines :
			
		if("Number of reads before trimming" in line):
			nbReads = line.split(' ')[6]
			nbReads = nbReads.rstrip()
			dicoResult["reads"] = nbReads

		if("(QC-passed reads + QC-failed reads)" in line):
			trimming = line.split(' ')[0]
			dicoResult["trimming"] = trimming	
			
		if(" mapped (" in line):
			mapped = line.split(' ')[0]
			dicoResult["mapped"] = mapped	

		if("properly paired" in line):
			paired = line.split(' ')[0]
			dicoResult["paired"] = paired
			
		if("Deep coverage" in line):
			deep = line.split(' ')[3]
			deep = deep.rstrip()
			dicoResult["deep cov."] = deep	

		if("Breadth coverage" in line):
			breadth = line.split(' ')[3]
			breadth = breadth.rstrip()
			dicoResult["breadth cov."] = breadth

	return dicoResult	


def numberOfVariant_from_VCF(VCF_file):

	vcfFile = open(VCF_file, "r")
	lines = vcfFile.readlines()
	vcfFile.close()

	SNPnb = 0

	for line in lines :
		if(line[0]!='#' and line.split('\t')[4]!='.'):
			SNPnb += 1

	return SNPnb		


def number_of_reads(reads_file):

	reads = open(reads_file, 'r')
	nbReads = len(reads.readlines())
	reads.close()

	return nbReads/4 #fastqfile = 4 lines per read


def MSLT_analysis(contigs, nbThreads, logFilepath, report, cwd):
	
	os.system('echo "\n************************** MLST WORKFLOW : START **************************\n" >> '+ logFilepath)

	command = "mlst --quiet " + " --threads=" + str(nbThreads) + \
	" " + contigs + " > MLST_result.tmp"

	os.system(command)


	#result parser

	MLST_result = open("MLST_result.tmp", 'r')
	lines = MLST_result.readlines()
	MLST_result.close()

	RESULT_ST = lines[0].split('\t')[2]

	
	# write report
	
	report_file = open(report, "a")
	report_file.write("" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+              MLST RESULTS            +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n")	
	report_file.write("Genus+Sp" + "\t" + "CC or ST" + "\t" + "Genes" + "\n")
	report_file.write(str(list(lines[0].split('\t'))[1:])) 

	if lines[0].split('\t')[1] == 'senterica' :
		salmo = True
	else:
		salmo = False
		

	RESULT_MLST = [RESULT_ST]	

	logFile = open(logFilepath,"a")

	logFile.close()
	report_file.close()

	os.system("cat MLST_result.tmp >> " + logFilepath)
	os.system("rm MLST_result.tmp")

	os.system('echo "*\n************************* MLST WORKFLOW : END **************************\n" >> '+ logFilepath)
	
	return RESULT_MLST, salmo


def get_serovar_from_ST_Salmonella(contigs,logFilepath, report, cwd, nbthread):

	output = cwd + "/SeqSero_ouptut"
	#cmd = "SeqSero2_package.py -m k -t 4 -i " + contigs + " -p " + nbthread + " -b mem -d " + output + " >> " + logFilepath
	cmd = "SeqSero.py -m 4 -i " + contigs + " -b mem -d " + output + " >> " + logFilepath
	os.system(cmd)
	
	#Parse SeqSero output
	SeqSero_file = open(output + "/Seqsero_result.txt", 'r')
	lines = SeqSero_file.readlines()
	line_serovar = 5
	if "Sdf prediction:" in lines[5] :
		line_serovar = 6
	try:
		#predictedSerovar = lines[7].rstrip().split('\t')[1] ## for seqsero2
		predictedSerovar = lines[line_serovar].rstrip().split('\t')[1]
		predictedSerovar = re.sub('\(.*','',predictedSerovar)
		predictedSerovar = predictedSerovar.replace('*','')
		if predictedSerovar == "potential monophasic variant of Typhimurium":
			predictedSerovar = "Typhimurium monophasic variant"
		elif ' or ' in predictedSerovar or 'potential ' in predictedSerovar :
			predictedSerovar = "N/A"
	except:
		predictedSerovar = "N/A"
		
	if "N/A" in predictedSerovar:
		predictedSerovar = "N/A"
		predictedAntigenicProfile = "N/A"

	else :
		#predictedAntigenicProfile = lines[6].rstrip().split('\t')[1]
		predictedAntigenicProfile = lines[4].rstrip().split('\t')[1]	
				
	#shutil.rmtree(output)
	os.system("rm -r " + output)

	report_file = open(report, 'a')
	report_file.write("\nPREDICTED SEROVAR : " + predictedSerovar + "\n")
	report_file.write("\nPREDICTED ANTIGENIC PROFILE : " + predictedAntigenicProfile + "\n")
	report_file.close()	
	
	return predictedSerovar, predictedAntigenicProfile


def Assembly(reads1,reads2,cwd,logFilepath,nbThreads): #Perform de novo assembly with Spades

	os.system('echo "\n************************** DE NOVO ASSEMBLY WORKFLOW : START **************************\n" >> '+ logFilepath)
	AssemblyDeNovopath = cwd + "/Assembly"

	os.system('echo "Assembly started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	cmd = "spades.py --careful " + \
	" -1 " + reads1 + " " + \
	" -2 " + reads2 + " " + \
	" -o " + AssemblyDeNovopath + \
	" -t "  + str(nbThreads) + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	os.system('echo "Assembly ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	copyfile(AssemblyDeNovopath + "/contigs.fasta",cwd + "/contigs.fasta")
	copyfile(AssemblyDeNovopath + "/scaffolds.fasta",cwd + "/scaffolds.fasta")
	
	os.system('echo "\n************************** DE NOVO ASSEMBLY WORKFLOW : END  **************************\n" >> '+ logFilepath)

	#return contig file path
	return cwd + "/contigs.fasta"


def Assembly_control(AssemblyDeNovo, referencepath, minContig, cwd, logFilepath, nbThreads):

	os.system('echo "\n************************** ASSEMBLY CONTROL : START **************************\n" >> '+ logFilepath)
	
	quast_html_file = cwd + "/QUAST" 

	### Start Quast ###
	os.system('echo "Quast started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	cmd = "quast " + \
	" -R " + referencepath.rstrip() + \
	" -o " + quast_html_file + \
	" -t " + str(nbThreads) + ' ' + \
	" -m " + str(minContig) + " " +  \
	" --split-scaffolds " + \
	AssemblyDeNovo + \
	" >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	os.system('echo "Quast ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	os.system('echo "\n************************** ASSEMBLY CONTROL : END **************************\n" >> '+ logFilepath)

	return quast_html_file


def TrimContigsByLen(fasta_file, min_length,logFilepath): #In order to do not keep "bad" contigs that potentially arent a part of this genome

    os.system('echo "\n************************** CONTIGS TRIMMING : START **************************\n" >> '+ logFilepath)
    os.system('echo "Contigs trimming started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

    # Stock the sequences in a dictionnary
    sequences={}

    # Biopython fasta parsing
    for seq_record in SeqIO.parse(fasta_file, "fasta"):

        # Take the current sequence
        sequence = str(seq_record.seq).upper()

        # Keep sequence accordint to its length
        if (len(sequence) >= min_length):
            sequences[sequence] = seq_record.id

    # Write non-trimmed sequences == filtered output
    trimmedcontigs = open(fasta_file + "_trimmed.fasta", "w+")
    # Just read the hash table and write on the file as a fasta format
    for sequence in sequences:
            trimmedcontigs.write(">" + sequences[sequence] + "\n" + sequence + "\n")
    trimmedcontigs.close()
    
    os.system('echo "Contigs trimming ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath )
    os.system('echo "\n************************** CONTIGS TRIMMING : END **************************\n" >> '+ logFilepath)

    #return trimmed contigs file path
    return fasta_file + "_trimmed.fasta"


def scaffolding(contigs_file, referencepath, Gid, logFilepath ,maxMemory, medusa_path, medusa_scripts_path):

	os.system('echo "\n************************** SCAFFOLDING WORKFLOW : START **************************\n" >> '+ logFilepath)

	if medusa_scripts_path[-1] != '/' :
		medusa_scripts_path = medusa_scripts_path + '/'

	Maxmemoryg=Get_maxMemoryg(maxMemory) 
	scaffold_file = Gid + "_scaffold.fasta"
	
	os.system('echo "Scaffolding started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath )

	cmd = "java " + Maxmemoryg + " -jar " + medusa_path + " -d -w2 -scriptPath " + medusa_scripts_path + \
		" -i " + contigs_file + " -o " + scaffold_file + " -f " + referencepath + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)	

	os.system('echo "Scaffolding ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath )

	os.system('echo "\n************************** SCAFFOLDING WORKFLOW : END **************************\n" >> '+ logFilepath)


	return scaffold_file	



def CloseGaps(scaffold, contigs, read1, read2, nbThreads, Gid, readsLen, logFilepath, report):

	os.system('echo "\n************************** CLOSING GAPS : START **************************\n" >> '+ logFilepath)

	#os.system("gunzip " + read1)
	#os.system("gunzip " + read2)

	#read1 = '.'.join(read1.split('.')[0:-1])
	#read2 = '.'.join(read2.split('.')[0:-1])

	os.system('echo "GMcloser started at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)
	
	cmd = "gmcloser -t " + scaffold + " -q " + contigs + " -l " + str(readsLen) + \
	" -p gmclos -r " + read1 + " " + read2 + " -n " + str(nbThreads) + " -bq phred33 "

	cmd = cmd + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	os.system('echo "GMcloser ended at : " >> '+ logFilepath + " 2>&1 | date >> " + logFilepath)

	###################

	CloseGapsFile = Gid + "_scaffold_closed.fasta"

	if os.path.isfile("gmclos.gapclosed.fa"):
		#rename gapclosed output file
		os.system("mv gmclos.gapclosed.fa " + CloseGapsFile)

	else :
		os.system('echo "ERROR GMcloser no result"  ')	
		CloseGapsFile = scaffold
		rep = open(report, 'a')
		rep.write("\nERROR GMcloser --> step skipping\n")
		rep.close()


	###################

	os.system("rm -r gmclos*")	

	os.system('echo "\n************************** CLOSING GAPS : END **************************\n" >> '+ logFilepath)
	return CloseGapsFile


def rename_contig(fasta_file, genome_id, logFilepath):

	os.system('echo "\n************************** RENAME CONTIGS : START **************************\n" >> '+ logFilepath)

	fasta = open(fasta_file, 'r')
	fasta_lines = fasta.readlines()
	fasta.close()

	rename_fastaFile_name = genome_id + "_assembly.fasta"
	rename_fastaFile = open(rename_fastaFile_name, 'w')

	i = 1
	for line in fasta_lines :
		if line[0] == '>':
			rename_fastaFile.write('>' + genome_id + '_' + str(i) + '\n')
			i+=1
		else :
			j = 0
			for n in line :

				if j%70==0 and j>0 :
					rename_fastaFile.write('\n')
			
				rename_fastaFile.write(n)
			
				j+=1	

	rename_fastaFile.close()

	os.system('echo "\n************************** RENAME CONTIGS : END **************************\n" >> '+ logFilepath)
	
	return rename_fastaFile_name	


def annotation(fasta_file, genome_id, nbThreads, logFilepath):

	os.system('echo "\n************************** GENOME ANNOTATION : START **************************\n" >> '+ logFilepath)

	cmd = "prokka  --addgenes --addmrna --prefix annotation" + \
	" -cpus " + str(nbThreads) + " " + fasta_file
	cmd = cmd + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	annotation_gbk = genome_id + ".gbk"
	annotation_gff = genome_id + ".gff"

	os.system("mv annotation/annotation.gbk " + annotation_gbk)
	os.system("mv annotation/annotation.gff " + annotation_gff)

	os.system('echo "\n************************** GENOME ANNOTATION : END **************************\n" >> '+ logFilepath)

	return annotation_gbk, annotation_gff


def get_len(fastqc_txt):

    readslength = subprocess.check_output("grep 'Sequence length' " + fastqc_txt + " | cut -f2 ",shell=True).rstrip('\n')
    readslength = readslength.split('-')[-1]
    return readslength

	
def check_scaffold_coverage(scaffold, reference, min_percent, nbThreads, report, log, max_percent_diff):

	fasta_dict = SeqIO.to_dict(SeqIO.parse(scaffold, "fasta"))
	fasta_len_dict = {}
	sum_scaffold_len = 0

	for seq in fasta_dict :
		scaffold_len = len(str(fasta_dict[seq].seq))
		fasta_len_dict[seq] = scaffold_len
		sum_scaffold_len += scaffold_len

	ref_dict = SeqIO.to_dict(SeqIO.parse(reference, "fasta"))
	sum_reference_len = 0
	for seq in ref_dict :
		ref_len = len(str(ref_dict[seq].seq))
		sum_reference_len += ref_len


	diff_longer = abs(100 - (float(sum_scaffold_len)/float(sum_reference_len) * 100))

	if (diff_longer > max_percent_diff) :
		logFile = open(log, 'a')
		reportFile = open(report, 'a')
		error = "ERROR : assembly length too different from the reference length " + \
		"(ref:" + str(sum_reference_len) + "pb | assembly:" + str(sum_scaffold_len) + "pb) !!!"
		print(error)
		logFile.write(error)
		reportFile.write(error)
		logFile.close()
		reportFile.close()

		os.system("rm -r temp reference_scaffolding Assembly \
			fastqc contigs.fasta* scaffolds.fasta stats mash_genome_db annotation trashReads.fastq \
			reference_gff *_scaffold_closed.fasta* *_paired_R*.fq *_R*.fastq *_iVARCAll")

		sys.exit(1)


	tmp_sam_file = "check_scaffold_coverage.sam"	

	os.system("bwa index " + reference)
	os.system("bwa mem -t " + str(nbThreads) + " " + reference + " " + scaffold + \
		" > " + tmp_sam_file)	

	os.system("rm " + reference + ".*")

	tmp_sam_file2 = "check_scaffold_coverage_mapped.sam"
	os.system("samtools view -S -F 4 " + tmp_sam_file + " > " + tmp_sam_file2)

	sam_file = open(tmp_sam_file2, 'r')
	lines = sam_file.readlines()
	sam_file.close()

	sum_mapped_scaffold = 0
	scaffold_mapped_lit = []

	for line in lines :

		if line[0] == '@' :
			continue

		scaffoldId = line.split('\t')[0]	

		if scaffoldId not in scaffold_mapped_lit :
			scaffold_mapped_lit.append(scaffoldId)
			sum_mapped_scaffold += fasta_len_dict[scaffoldId]

	os.remove(tmp_sam_file)
	os.remove(tmp_sam_file2)
	print(sum_mapped_scaffold)
	print(sum_scaffold_len)
	mapped_percent = float(sum_mapped_scaffold) / float(sum_scaffold_len) * 100.0

	if mapped_percent >= min_percent :
		return

	else :
		logFile = open(log, 'a')
		reportFile = open(report, 'a')
		error = "ERROR : only " + str(mapped_percent) + " percent map on " + reference + " !!!"
		print(error)
		logFile.write(error)
		reportFile.write(error)
		logFile.close()
		reportFile.close()

		os.system("rm -r temp reference_scaffolding Assembly \
			fastqc contigs.fasta* scaffolds.fasta stats mash_genome_db annotation trashReads.fastq \
			reference_gff *_scaffold_closed.fasta* *_paired_R*.fq *_R*.fastq *_iVARCAll")

		sys.exit(1)


def make_mash_sketch(assembly, nbThreads, genome_id, Kmer_size, Sketch_size, logFilepath):

	cmd = "mash sketch -p " + str(nbThreads) + " -k " + Kmer_size + " -s " + Sketch_size + " -r -o " + genome_id + " " + assembly
	cmd = cmd + " >> " + logFilepath + " 2>&1"

	logFile = open(logFilepath, "a")
	logFile.write("\n" + cmd + "\n")
	logFile.close()

	os.system(cmd)

	return genome_id + ".msh"


def parse_reports(SampleID, cwd):

	artwork_report =  cwd + '/' + SampleID + "_report.txt"
	quast_report = cwd + "/QUAST/report.tsv"

	Dico_infos = {}

	#Parse ARTwort report
	artwork_file = open (artwork_report, 'r')
	artwork_content = artwork_file.readlines()
	artwork_file.close()

	for line in artwork_content :
		line = line.rstrip()
		line = line.split(' : ')
		# get ref_SNP
		if line[0] == 'REFERENCE GENOME' :
			Dico_infos["RefSNP"] = line[1]

    	# get breadth_cov
		elif line[0] == 'BREADTH COVERAGE' :
			Dico_infos["BreadthCoverage"] = line[1].split('%')[0]

    	# get ref_scafold
		elif line[0] == 'Reference used for scaffolding' :
			Dico_infos["RefScafold"] = line[1].split('.')[0]

	#Parse QUAST report
	quast_file = open (quast_report, 'r')
	quast_content = quast_file.readlines()
	quast_file.close()

	for line in quast_content :
		line = line.rstrip()
		line = line.split('\t')

	    # get nb contigs
		if line[0] == '# contigs (>= 0 bp)' :
			Dico_infos["NbContigs"] = line[1]

	    # get total_length
		elif line[0] == 'Total length (>= 0 bp)' :
			Dico_infos["TotalLength"] = line[1]

		# get largest_contig
		elif line[0] == 'Largest contig' :
			Dico_infos["LargestContig"] = line[1]

	    # get N50
		elif line[0] == 'N50' :
			Dico_infos["N50"] = line[1]

    	# genome_fraction
		elif line[0] == 'Genome fraction (%)' :
			Dico_infos["GenomeFraction"] = line[1]

	return Dico_infos


#main function	
def main():	


	##########################################
	#			Initialisation				 #
	##########################################

	# Processing time counter
	t0 = time.time()
	
	# Get arguments 
	parser=get_parser()
	
	# Print parser.help if no arguments
	if len(sys.argv)==1:
		parser.print_help()
		sys.exit(1)
	
	Arguments=parser.parse_args()	
	
	#itsmeludo 23/12/2018
        
        if not Arguments.medusa_path:
                medusa_path=os.path.join(os.environ['CONDA_PREFIX'],"bin","medusa.jar")
        else:
                medusa_path=Arguments.medusa_path
        if not Arguments.medusa_scripts_path:
                medusa_scripts_path=os.path.join(os.environ['CONDA_PREFIX'],"bin","medusa_scripts")
        else:
                medusa_scripts_path=Arguments.medusa_scripts_path
        ###fin modif
	#itsmeludo 27/12/2018
        if not Arguments.GATK_path:
                gatk_path=os.path.join(os.environ['CONDA_PREFIX'],"bin","GenomeAnalysisTK.jar")
        else:
                gatk_path=Arguments.GATK_path
	###fin modif	
    
	print("Step 1/16 ---> Initialisation")

	# Go to working directory gived by Arguments.workdir
	if(Arguments.workdir != '.'):
		os.chdir(Arguments.workdir)	

	cwd = os.getcwd() #Current working directory FULL path	

	# GLOBAL VARIABLES
	GENOME_ID = '_'.join(os.path.basename(Arguments.reads1).split('.')[0].split('_')[0:-1])
	LOG_FILE = cwd + "/" + GENOME_ID + ".log"
	REPORT_FILE = GENOME_ID + "_report.txt"
	FINAL_OUTPUT_DIRECTORY = "artwork_results_for_" + GENOME_ID 
	ADAPTATERS_FILE = Arguments.ADAPTATERS

	#Retrieve Python command
	command=""
	for arg in sys.argv:
		command = command + " " + arg	

	# First write in Log File : date and command
	LOG = open(LOG_FILE, 'w')
	LOG.write("READSPROCESS STARTED :" + time.strftime("%d/%m/%Y") + ' ' + time.strftime("%H:%M:%S") + '\n\n')
	LOG.write("COMMAND USED :" + command + '\n\n\n')
	LOG.close()

	# Check GENOME_ID
	check_finalDirectory_exist(FINAL_OUTPUT_DIRECTORY, LOG_FILE)

	# Make final directory 
	os.makedirs(FINAL_OUTPUT_DIRECTORY)

	# First write in Report file : Metadata and key processing parameters
	REPORT = open(REPORT_FILE, 'w')
	REPORT.write("++++++++++++++++++++++++++++++++++++++++ DATASET REPORT  ++++++++++++++++++++++++++++++++++++++++\n" + \
	"GENERATED " + time.strftime("%d/%m/%Y") + ' AT ' + time.strftime("%H:%M:%S") + '\n' + \
	"READS : \n" + Arguments.reads1.split('/')[-1] + "\n" + Arguments.reads2.split('/')[-1] + '\n' + \
	"\n"+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+             KEY PARAMETERS           +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n"+ \
	"Coverage threshold : " + str(Arguments.minCov) + '\n' + \
	"Contig length threshold : " + str(Arguments.minContigLen) + '\n' + \
	"Phred score threshold : " + str(Arguments.minPhredScore) + '\n' + \
	"\n"+ \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"+              MAIN METRICS            +\n" + \
	"++++++++++++++++++++++++++++++++++++++++ \n" + \
	"\n")
	REPORT.close()


	##########################################
	#				Check reads				 #
	##########################################
	
	print("Step 2/16 ---> Check reads format")

	### READ 1 ###

	# If file isn't in the right format, analyse it and uncompress it if possible. 
	if(Arguments.reads1.split('.')[-1] not in ["fastq", "fq", "FASTQ", "FQ"]):
		Arguments.reads1,IS_corrupted = uncompress_in_workdir(Arguments.reads1, LOG_FILE,cwd)

		if (IS_corrupted==True):
			# ASTUCE : utiliser booleen IS_corrupted: En considerant que le script sera appelé pour chaque paire de reads, sauter le traitement du reads pair si le premier est déjà corrompu
			sys.exit(1) 

	# File is already in the right format : copy it in the current work directory		
	else:
		copyfile(Arguments.reads1, cwd + "/" + list(Arguments.reads1.rsplit('/',1))[1])
		Arguments.reads1= cwd + '/' + Arguments.reads1.split('/')[-1]
		LOG=open(LOG_FILE,"a")
		LOG.write("Reads already uncompressed in:" + Arguments.reads1 + "\n")
		LOG.close()


	### READ 2 : Same procedure (if read1 processed successfully, according to IS_corrupted ) ###


	if(Arguments.reads2.split('.')[-1] not in ["fastq", "fq", "FASTQ", "FQ"]):
		Arguments.reads2,IS_corrupted = uncompress_in_workdir(Arguments.reads2, LOG_FILE,cwd)
		
		if (IS_corrupted==True):
			sys.exit(1)

	else:
		copyfile(Arguments.reads2, cwd + "/" + list(Arguments.reads2.rsplit('/',1))[1])
		Arguments.reads2 = cwd + '/' + Arguments.reads2.split('/')[-1]	
		LOG=open(LOG_FILE,"a")
		LOG.write("reads already uncompressed in:" + Arguments.reads2)
		LOG.close()	


	##########################################
	#		    Coverage checking			 #
	##########################################
	
	print("Step 3/16 ---> quality control")
	coverage = Check_coverage(Arguments.reads1,Arguments.reads2,Arguments.minCov,Arguments.nbThreads,Arguments.maxMemory,\
	cwd,LOG_FILE,REPORT_FILE, Arguments.RefScaffold)


	##########################################
	#		      Normalization 			 #
	##########################################
	
	print("Step 4/17 ---> normalization")
	if(coverage > Arguments.targetCov):
		reads1, reads2 = normalization(Arguments.reads1, Arguments.reads2, Arguments.targetCov, Arguments.nbThreads, GENOME_ID, LOG_FILE, REPORT_FILE)
		os.system("mv " + reads1 + " " + Arguments.reads1)
		os.system("mv " + reads2 + " " + Arguments.reads2)

	##########################################
	#		    	  FASTQC		    	 #
	##########################################

	print("Step 5/17 ---> FASTQC")

	fastqcResult = Fastqc(Arguments.reads1, Arguments.reads2, ADAPTATERS_FILE, "fastqc", Arguments.nbThreads, cwd, LOG_FILE, REPORT_FILE)

	FASTQC_R1 = fastqcResult[0]
	FASTQC_R2 = fastqcResult[1]
	fastqc_txtreport = fastqcResult[2]


	##########################################
	#		    	iVARCall		    	 #
	##########################################

	print("Step 6/17 ---> iVARCall2")

	VCF_FILE = commande_iVARCAll(Arguments.reads1,Arguments.reads2, ADAPTATERS_FILE,\
		Arguments.nbThreads, Arguments.maxMemory, GENOME_ID,cwd, LOG_FILE, REPORT_FILE, Arguments.RefVariant, \
		GATK_path=gatk_path)
#	VCF_FILE = commande_iVARCAll(Arguments.genus, Arguments.reads1,Arguments.reads2, ADAPTATERS_FILE,\
#                Arguments.nbThreads, Arguments.maxMemory, GENOME_ID,cwd, LOG_FILE, REPORT_FILE, Arguments.NASBIO1_PATH, \
#                Arguments.trimmomatic_path, Arguments.GATK_path, Arguments.picard_path)


	##########################################
	#		    Trimming reads		    	 #
	##########################################

	print("Step 7/17 ---> Trimming")
	reads_trim =  soft_trimming(Arguments.reads1, Arguments.reads2, ADAPTATERS_FILE, Arguments.nbThreads, Arguments.maxMemory, \
	Arguments.minPhredScore, GENOME_ID, cwd, REPORT_FILE, LOG_FILE)
	
	reads1_trim = reads_trim[0]
	reads2_trim = reads_trim[1]


	##########################################
	#	        De novo assembly        	 #
	##########################################	

	print("Step 8/17 ---> De novo assembly")

	CONTIGS = Assembly(reads1_trim,reads2_trim,cwd,LOG_FILE,Arguments.nbThreads)


	##########################################
	#	              MLST	    	    	 #
	##########################################

	print("Step 9/17 ---> MLST")

	MLST_result, salmo = MSLT_analysis(CONTIGS, Arguments.nbThreads, LOG_FILE, REPORT_FILE, cwd)

	##########################################
	#	           Scaffolding   	         #
	##########################################

	print("Step 10/17 ---> Scaffolding")

	os.makedirs("ref_scafold")
	os.system("cp " + Arguments.RefScaffold + " ref_scafold/.")
	scaffold = scaffolding(CONTIGS, "ref_scafold", GENOME_ID, LOG_FILE, Arguments.maxMemory, \
	medusa_path , medusa_scripts_path)
	os.system("rm -r ref_scafold")

	##########################################
	#	          Gap Closing	             #
	##########################################	

	print("Step 11/17 ---> Gap closing")

	readsLen = get_len(str(fastqc_txtreport))
	scaffold = CloseGaps(scaffold, CONTIGS, reads1_trim, reads2_trim, Arguments.nbThreads, GENOME_ID, readsLen, LOG_FILE, REPORT_FILE)

	##########################################
	#	         Contig filtration     	 	 #
	##########################################

	print("Step 12/17 ---> Contig filtration")

	scaffold = TrimContigsByLen(scaffold, Arguments.minContigLen, LOG_FILE)


	##########################################
	#	      QUAST quality control     	 #
	##########################################

	print("Step 13/17 ---> Assembly control")

	check_scaffold_coverage(scaffold, Arguments.RefScaffold, Arguments.min_mapped_scaffold_percent, Arguments.nbThreads, REPORT_FILE, LOG_FILE, \
	Arguments.max_len_diff)
	QUAST = Assembly_control(scaffold, Arguments.RefScaffold, Arguments.minContigLen, cwd, LOG_FILE, Arguments.nbThreads)
	

	##########################################
	#	        Rename scaffolds           	 #
	##########################################

	print("Step 14/17 ---> Rename scaffold and make mash sketch")

	ASSEMBLY = rename_contig(scaffold, GENOME_ID, LOG_FILE)

	SKETCH = make_mash_sketch(ASSEMBLY, Arguments.nbThreads, GENOME_ID, Arguments.Ksize, Arguments.Ssize, LOG_FILE)


	##########################################
	#	        	Annotation            	 #
	##########################################

	print("Step 15/17 ---> Genome annotation")

	GBK, GFF = annotation(ASSEMBLY, GENOME_ID, Arguments.nbThreads, LOG_FILE)


	##########################################
	#	          Save results          	 #
	##########################################

	print("Step 16/17 ---> Moving files and add to mongoDB")

	dico_Suplinfos = parse_reports(GENOME_ID, cwd)
	
	if salmo :
		RESULT_SEROVAR,RESULT_ANTIGENIC_PROFILE = get_serovar_from_ST_Salmonella(ASSEMBLY,LOG_FILE, REPORT_FILE, cwd, Arguments.nbThreads)
		MLST_id = [RESULT_SEROVAR, RESULT_ANTIGENIC_PROFILE]

	sampleObjet = sample(GENOME_ID, REPORT_FILE, \
		Arguments.reads1, Arguments.reads2, readsLen, FASTQC_R1, FASTQC_R2, \
		ASSEMBLY, CONTIGS, QUAST, GBK, GFF,\
		VCF_FILE, SKETCH, dico_Suplinfos['NbContigs'], \
		dico_Suplinfos['TotalLength'], dico_Suplinfos['LargestContig'], \
		dico_Suplinfos['N50'], dico_Suplinfos['GenomeFraction'], Arguments.RefScaffold, dico_Suplinfos['RefSNP'], \
		dico_Suplinfos['BreadthCoverage'])

	sampleObjet.moveFiles(FINAL_OUTPUT_DIRECTORY)


	##########################################
	#	        Remove tmp files	       	 #
	##########################################

	print("Step 17/17 ---> Cleaning temporary files")

	# Last write in log file : date and time elapsed
	t1 = time.time() - t0
	LOG = open(LOG_FILE, 'a')
	LOG.write(" \n\n\n READSPROCESS ENDED :" + time.strftime("%d/%m/%Y") + ' ' + time.strftime("%H:%M:%S") + "\n\n")
	LOG.write(" TIME ELAPSED (hour:min:sec) :" + str(datetime.timedelta(seconds=int(t1))))
	LOG.close()

	os.system("mv " + GENOME_ID + ".log " + FINAL_OUTPUT_DIRECTORY + "/.")

	# Remove tmp files and directories
	os.system("rm -r  " + GENOME_ID + "* temp reference_scaffolding Assembly \
		fastqc contigs.fasta* scaffolds.fasta stats mash_genome_db annotation trashReads.fastq \
		reference_gff SeqSero_output")


if __name__ == "__main__":
	main()	        		
